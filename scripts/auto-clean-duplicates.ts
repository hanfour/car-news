#!/usr/bin/env tsx

import * as dotenv from 'dotenv'
import * as path from 'path'
dotenv.config({ path: path.resolve(process.cwd(), '.env.local') })

import { createServiceClient } from '../src/lib/supabase'
import {
  findSemanticDuplicatesInBatch,
  findKeywordDuplicatesInBatch,
  pairsToDuplicateGroups,
  ArticleForDuplicateCheck,
  DuplicateGroup
} from '../src/lib/utils/advanced-deduplication'

/**
 * è‡ªå‹•æ¸…ç†é‡è¤‡æ–‡ç« è…³æœ¬
 *
 * åŠŸèƒ½ï¼š
 * 1. æƒææœ€è¿‘ 7 å¤©çš„æ–‡ç« 
 * 2. æª¢æ¸¬èªç¾©é‡è¤‡ï¼ˆembedding > 90%ï¼‰å’Œé—œéµè©é‡è¤‡ï¼ˆ> 70%ï¼‰
 * 3. è‡ªå‹•ä¸‹ç·šè¼ƒæ–°çš„é‡è¤‡æ–‡ç« ï¼ˆä¿ç•™æœ€æ—©ç™¼å¸ƒçš„ï¼‰
 * 4. ç”Ÿæˆæ¸…ç†å ±å‘Š
 */

async function findSemanticDuplicates(): Promise<DuplicateGroup[]> {
  const supabase = createServiceClient()

  console.log('ğŸ” æƒæèªç¾©é‡è¤‡ï¼ˆEmbedding Similarity > 90%ï¼‰...')

  const sevenDaysAgo = new Date()
  sevenDaysAgo.setDate(sevenDaysAgo.getDate() - 7)

  const { data: articles } = await supabase
    .from('generated_articles')
    .select('id, title_zh, created_at, published, view_count, content_embedding, primary_brand')
    .gte('created_at', sevenDaysAgo.toISOString())
    .not('content_embedding', 'is', null)
    .order('created_at', { ascending: true })

  if (!articles || articles.length === 0) {
    console.log('   ç„¡æ–‡ç« å¯æª¢æŸ¥')
    return []
  }

  console.log(`   æª¢æŸ¥ ${articles.length} ç¯‡æ–‡ç« ...`)

  // ä½¿ç”¨å…±ç”¨å‡½æ•¸æª¢æ¸¬é‡è¤‡å°
  const articlesForCheck: ArticleForDuplicateCheck[] = articles.map(a => ({
    id: a.id,
    title_zh: a.title_zh,
    created_at: a.created_at,
    published: a.published,
    view_count: a.view_count || 0,
    content_embedding: a.content_embedding,
    primary_brand: a.primary_brand
  }))

  const pairs = findSemanticDuplicatesInBatch(articlesForCheck, 0.90)
  const duplicateGroups = pairsToDuplicateGroups(pairs, articlesForCheck, 'semantic')

  console.log(`   âœ“ ç™¼ç¾ ${duplicateGroups.length} çµ„èªç¾©é‡è¤‡`)
  return duplicateGroups
}

async function findKeywordDuplicates(): Promise<DuplicateGroup[]> {
  const supabase = createServiceClient()

  console.log('\nğŸ” æƒæé—œéµè©é‡è¤‡ï¼ˆKeyword Overlap > 70%ï¼‰...')

  const sevenDaysAgo = new Date()
  sevenDaysAgo.setDate(sevenDaysAgo.getDate() - 7)

  const { data: articles } = await supabase
    .from('generated_articles')
    .select('id, title_zh, created_at, published, view_count, primary_brand')
    .gte('created_at', sevenDaysAgo.toISOString())
    .not('primary_brand', 'is', null)
    .order('created_at', { ascending: true })

  if (!articles || articles.length === 0) {
    console.log('   ç„¡æ–‡ç« å¯æª¢æŸ¥')
    return []
  }

  console.log(`   æª¢æŸ¥ ${articles.length} ç¯‡æ–‡ç« ...`)

  // ä½¿ç”¨å…±ç”¨å‡½æ•¸æª¢æ¸¬é‡è¤‡å°
  const articlesForCheck: ArticleForDuplicateCheck[] = articles.map(a => ({
    id: a.id,
    title_zh: a.title_zh,
    created_at: a.created_at,
    published: a.published,
    view_count: a.view_count || 0,
    primary_brand: a.primary_brand
  }))

  const pairs = findKeywordDuplicatesInBatch(articlesForCheck, 0.70, 2)
  const duplicateGroups = pairsToDuplicateGroups(pairs, articlesForCheck, 'keyword')

  console.log(`   âœ“ ç™¼ç¾ ${duplicateGroups.length} çµ„é—œéµè©é‡è¤‡`)
  return duplicateGroups
}

async function unpublishDuplicates(groups: DuplicateGroup[], dryRun: boolean = true): Promise<{
  unpublished: number
  kept: number
}> {
  const supabase = createServiceClient()
  let unpublishedCount = 0
  let keptCount = 0

  console.log(`\n${'='.repeat(80)}`)
  console.log(`\nğŸ“Š è™•ç†é‡è¤‡æ–‡ç« ï¼ˆ${dryRun ? 'DRY RUN - ä¸å¯¦éš›åŸ·è¡Œ' : 'æ­£å¼åŸ·è¡Œ'}ï¼‰\n`)

  for (let i = 0; i < groups.length; i++) {
    const group = groups[i]

    console.log(`[çµ„ ${i + 1}/${groups.length}] ${group.type === 'semantic' ? 'èªç¾©' : 'é—œéµè©'}é‡è¤‡ (${(group.similarity * 100).toFixed(1)}%)`)

    if (group.keywords && group.keywords.length > 0) {
      console.log(`  å…±åŒé—œéµè©: ${group.keywords.join(', ')}`)
    }

    // æ’åºï¼šä¿ç•™æœ€æ—©å‰µå»ºçš„æ–‡ç« 
    const sorted = [...group.articles].sort((a, b) =>
      new Date(a.created_at).getTime() - new Date(b.created_at).getTime()
    )

    const toKeep = sorted[0]
    const toUnpublish = sorted.slice(1).filter(a => a.published)

    console.log(`  âœ… ä¿ç•™: [${toKeep.id}] ${toKeep.title_zh}`)
    console.log(`     å‰µå»º: ${toKeep.created_at} | ç€è¦½: ${toKeep.view_count}`)

    keptCount++

    for (const article of toUnpublish) {
      console.log(`  âŒ ä¸‹ç·š: [${article.id}] ${article.title_zh}`)
      console.log(`     å‰µå»º: ${article.created_at} | ç€è¦½: ${article.view_count}`)

      if (!dryRun) {
        const { error } = await supabase
          .from('generated_articles')
          .update({ published: false, published_at: null })
          .eq('id', article.id)

        if (error) {
          console.log(`     âš ï¸  ä¸‹ç·šå¤±æ•—: ${error.message}`)
        } else {
          console.log(`     âœ“ å·²ä¸‹ç·š`)
          unpublishedCount++
        }
      } else {
        unpublishedCount++
      }
    }

    console.log('')
  }

  return { unpublished: unpublishedCount, kept: keptCount }
}

async function generateReport(
  semanticGroups: DuplicateGroup[],
  keywordGroups: DuplicateGroup[],
  stats: { unpublished: number; kept: number },
  dryRun: boolean
) {
  console.log(`${'='.repeat(80)}`)
  console.log('\nğŸ“‹ æ¸…ç†å ±å‘Š\n')

  console.log(`æª¢æ¸¬æ™‚é–“: ${new Date().toLocaleString('zh-TW')}`)
  console.log(`æ¨¡å¼: ${dryRun ? 'æ¨¡æ“¬åŸ·è¡Œï¼ˆæœªå¯¦éš›ä¿®æ”¹ï¼‰' : 'æ­£å¼åŸ·è¡Œ'}`)

  console.log('\næª¢æ¸¬çµæœ:')
  console.log(`  èªç¾©é‡è¤‡çµ„: ${semanticGroups.length}`)
  console.log(`  é—œéµè©é‡è¤‡çµ„: ${keywordGroups.length}`)
  console.log(`  ç¸½é‡è¤‡çµ„: ${semanticGroups.length + keywordGroups.length}`)

  console.log('\nè™•ç†çµæœ:')
  console.log(`  âœ… ä¿ç•™æ–‡ç« : ${stats.kept}`)
  console.log(`  âŒ ä¸‹ç·šæ–‡ç« : ${stats.unpublished}`)

  const totalArticles = stats.kept + stats.unpublished
  const deduplicationRate = totalArticles > 0 ? ((stats.unpublished / totalArticles) * 100).toFixed(1) : '0.0'
  console.log(`  ğŸ“Š é‡è¤‡ç‡: ${deduplicationRate}%`)

  console.log('\nå»ºè­°:')
  if (semanticGroups.length > 0 || keywordGroups.length > 0) {
    console.log('  â€¢ æª¢æŸ¥ Generator çš„é˜²é‡æ©Ÿåˆ¶æ˜¯å¦æ­£å¸¸é‹ä½œ')
    console.log('  â€¢ èª¿æ•´ Topic Lock çš„æœ‰æ•ˆæœŸé™')
    console.log('  â€¢ è€ƒæ…®æé«˜ embedding ç›¸ä¼¼åº¦é–¾å€¼')
  } else {
    console.log('  â€¢ âœ… é˜²é‡æ©Ÿåˆ¶é‹ä½œæ­£å¸¸ï¼Œç„¡éœ€èª¿æ•´')
  }

  console.log(`\n${'='.repeat(80)}`)
}

async function main() {
  const args = process.argv.slice(2)
  const dryRun = !args.includes('--execute')

  console.log('ğŸ§¹ è‡ªå‹•æ¸…ç†é‡è¤‡æ–‡ç« \n')
  console.log(`æ¨¡å¼: ${dryRun ? 'ğŸ” DRY RUNï¼ˆæ¨¡æ“¬åŸ·è¡Œï¼Œä¸å¯¦éš›ä¿®æ”¹ï¼‰' : 'âš¡ EXECUTEï¼ˆæ­£å¼åŸ·è¡Œï¼‰'}`)

  if (dryRun) {
    console.log('æç¤º: ä½¿ç”¨ --execute åƒæ•¸åŸ·è¡Œå¯¦éš›æ¸…ç†\n')
  } else {
    console.log('âš ï¸  è­¦å‘Š: å°‡å¯¦éš›ä¸‹ç·šé‡è¤‡æ–‡ç« ï¼\n')
    await new Promise(resolve => setTimeout(resolve, 3000))
  }

  console.log(`${'='.repeat(80)}\n`)

  // 1. æª¢æ¸¬èªç¾©é‡è¤‡
  const semanticGroups = await findSemanticDuplicates()

  // 2. æª¢æ¸¬é—œéµè©é‡è¤‡
  const keywordGroups = await findKeywordDuplicates()

  // 3. è™•ç†é‡è¤‡
  const allGroups = [...semanticGroups, ...keywordGroups]

  if (allGroups.length === 0) {
    console.log('\nâœ… æ²’æœ‰ç™¼ç¾é‡è¤‡æ–‡ç« ï¼')
    console.log(`\n${'='.repeat(80)}`)
    return
  }

  const stats = await unpublishDuplicates(allGroups, dryRun)

  // 4. ç”Ÿæˆå ±å‘Š
  await generateReport(semanticGroups, keywordGroups, stats, dryRun)

  if (dryRun) {
    console.log('\nğŸ’¡ é€™åªæ˜¯æ¨¡æ“¬åŸ·è¡Œã€‚ä½¿ç”¨ --execute åƒæ•¸ä¾†å¯¦éš›åŸ·è¡Œæ¸…ç†ã€‚')
  } else {
    console.log('\nâœ… æ¸…ç†å®Œæˆï¼')
  }
}

main().catch(error => {
  console.error('âŒ åŸ·è¡Œå¤±æ•—:', error)
  process.exit(1)
})
